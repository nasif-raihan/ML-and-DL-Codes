{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_mnist.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNwhp0KlFb0loP+2vMUHaR+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nasif-raihan/ML-and-DL-Codes/blob/main/GAN_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Resorce](https://github.com/bnsreenu/python_for_microscopists/blob/master/125_126_GAN_training_mnist.py)"
      ],
      "metadata": {
        "id": "9zgdLm5QGZE9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rOd2gKxGzyt8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.datasets import mnist \n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten\n",
        "from tensorflow.keras.layers import LeakyReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define input image dimensions\n",
        "Large images take too much time and resources.**"
      ],
      "metadata": {
        "id": "m6GmqJPTKDj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "\n",
        "img_shape = (img_rows, img_cols, channels)"
      ],
      "metadata": {
        "id": "rZR0S4Sl0bj3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Given input of noise (latent) vector, the Generator produces an image.**\n",
        "\n",
        "**Alpha — α is a hyperparameter which controls the underlying value to which the\n",
        "function saturates negatives network inputs.**\n",
        "\n",
        "**Momentum — Speed up the training**\n",
        "\n",
        "**Define your generator network.\n",
        "Here we are only using Dense layers. But network can be complicated based on \n",
        "the application. For example, you can use VGG for super res. GAN.**"
      ],
      "metadata": {
        "id": "8Eu7pfIEJs28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator():\n",
        "  noise_shape = (100, )   # 1D array of size 100 (latent vector / noise)\n",
        "\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Input(shape=(noise_shape)))\n",
        "\n",
        "  model.add(Dense(256))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(1025))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
        "  model.add(Reshape(img_shape))\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  noise = Input(shape=noise_shape)\n",
        "  img = model(noise)  # Generated Image\n",
        "\n",
        "  return Model(noise, img)"
      ],
      "metadata": {
        "id": "VcVDlKHH0bYb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Given an input image, the Discriminator outputs the likelihood of the image being real.**\n",
        "  \n",
        "**Binary classification - true or false (we're calling it validity)**"
      ],
      "metadata": {
        "id": "lhf8yKi3Jmrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Flatten(input_shape=img_shape))\n",
        "\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Dense(256))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  img = Input(shape=img_shape)\n",
        "  validity = model(img)   #The validity is the Discriminator’s guess of input being real or not.\n",
        "\n",
        "  return Model(img, validity)"
      ],
      "metadata": {
        "id": "NSfGL2R40bPw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the Data:**"
      ],
      "metadata": {
        "id": "ZA_iI2gHz0XK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, _), (_, _) = mnist.load_data()\n",
        "print(X_train.shape)\n",
        "\n",
        "# Convert to float and Rescale -1 to 1 (Can also do 0 to 1)\n",
        "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "# Add channels dimension. As the input to our generator and discriminator has a shape 28x28x1.\n",
        "X_train = np.expand_dims(X_train, axis=3)\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzydB1ODN44S",
        "outputId": "fb1812e8-7061-4252-bb7b-45b51fc2c2ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now that we have constructed our two models it’s time to pit them against each other.**\n",
        "\n",
        "**We do this by defining a training function, loading the data set, re-scaling our training images and setting the ground truths.**\n",
        "\n",
        "***Epochs*** dictate the number of backward and forward propagations.\n",
        "\n",
        "***Batch_size*** indicates the number of training samples per backward/forward propagation.\n",
        "\n",
        "***sample_interval*** specifies after how many epochs we call our sample_image function."
      ],
      "metadata": {
        "id": "AaGP0M7BJOjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs, batch_size=128, save_interval=50):\n",
        "  half_batch = int(batch_size / 2)\n",
        "  \"\"\"  \n",
        "    We loop through a number of epochs to train our \n",
        "    Discriminator by first selecting a random batch of images from our true dataset, \n",
        "    generating a set of images from our Generator, \n",
        "    feeding both set of images into our Discriminator, and \n",
        "    finally setting the loss parameters for both the real and fake images, \n",
        "    as well as the combined loss.\n",
        "  \"\"\" \n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    # ---------------------\n",
        "    #  Train Discriminator\n",
        "    # ---------------------\n",
        "\n",
        "    # Select a random half batch of real images\n",
        "    # random.randint(low, high=None, size=None, dtype=int)\n",
        "    idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "    imgs = X_train[idx]\n",
        "\n",
        "    # random.normal(loc=0.0, scale=1.0, size=(int or tuple of ints, optional))  \n",
        "    # Returns: ndarray or scalar\n",
        "    noise = np.random.normal(0, 1, (half_batch, 100))\n",
        "\n",
        "    # Generate a half batch of fake images\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    \"\"\"\n",
        "      Train the discriminator on real and fake images, separately.\n",
        "      Research showed that separate training is more effective.\n",
        "    \"\"\"\n",
        "\n",
        "    d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
        "\n",
        "    # Take average loss from real and fake images.\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "    \n",
        "    \"\"\"\n",
        "      And within the same loop we train our Generator, by setting the input noise and\n",
        "      ultimately training the Generator to have the Discriminator label its samples as valid\n",
        "      by specifying the gradient loss.\n",
        "    \"\"\"\n",
        "    # ---------------------\n",
        "    #  Train Generator\n",
        "    # ---------------------\n",
        "\n",
        "    \"\"\"\n",
        "      Create noise vectors as input for generator. \n",
        "      Create as many noise vectors as defined by the batch size. \n",
        "      Based on normal distribution. Output will be of size (batch size, 100)\n",
        "    \"\"\"\n",
        "\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "\n",
        "    \"\"\"\n",
        "      The generator wants the discriminator to label the generated samples as valid (ones)\n",
        "      This is where the genrator is trying to trick discriminator into believing the generated image is true (hence value of 1 for y)\n",
        "    \"\"\"\n",
        "\n",
        "    valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
        "    \n",
        "    \"\"\"\n",
        "        Generator is a part of combined where it got directly linked with the discriminator\n",
        "        Train the generator with noise as x and 1 as y. \n",
        "        Again, 1 as the output as it is adversarial and if generator did a great\n",
        "        job of fooling the discriminator then the output would be 1 (true)\n",
        "    \"\"\"\n",
        "\n",
        "    g_loss = combined.train_on_batch(noise, valid_y)\n",
        "\n",
        "    \"\"\"\n",
        "      Additionally, in order for us to keep track of our training process, we print the\n",
        "      progress and save the sample image output depending on the epoch interval specified.  \n",
        "      Plot the progress.\n",
        "    \"\"\"\n",
        "    \n",
        "    print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "    # If at save interval => save generated image samples\n",
        "    if epoch % save_interval == 0:\n",
        "        save_imgs(epoch)\n"
      ],
      "metadata": {
        "id": "9hkpCh-10bIN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When the specific sample_interval is hit, we call the sample_image function. Which looks as follows:**"
      ],
      "metadata": {
        "id": "K7Gt4Bv4JI67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This function saves our images for us to view\n",
        "\n",
        "def save_imgs(epoch):\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, 100))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "r73Cwcxb0a-_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us also define our optimizer for easy use later on.\n",
        "That way if you change your mind, you can change it easily here."
      ],
      "metadata": {
        "id": "3c-nPdnkLw_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(0.0002, 0.5)  #Learning rate and momentum."
      ],
      "metadata": {
        "id": "kU7W4jeJ0az7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build and compile the discriminator first.**\n",
        "\n",
        "**Generator will be trained as part of the combined model, later.**\n",
        "\n",
        "**Pick the loss function and the type of metric to keep track.**                 \n",
        "\n",
        "**Binary cross entropy as we are doing prediction and it is a better loss function compared to MSE or other.** "
      ],
      "metadata": {
        "id": "EGgYJ-8OMINI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build and compile our Discriminator, pick the loss function."
      ],
      "metadata": {
        "id": "6vtDKJSnM3Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "3n5XCSHC0alz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3efbcc0a-7ff0-4f37-be9d-81fce3b4fbb8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               401920    \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are only generating (faking) images, let us not track any metrics."
      ],
      "metadata": {
        "id": "RzUYZ75zNK4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = build_generator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO0wTqJBvDww",
        "outputId": "7a3f08a7-a211-4364-f207-264a4ecd1e1f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 256)               25856     \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 256)              1024      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               131584    \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1025)              525825    \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 1025)              0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 1025)             4100      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 784)               804384    \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,494,821\n",
            "Trainable params: 1,491,235\n",
            "Non-trainable params: 3,586\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = build_generator()\n",
        "generator.compile(loss='binary_crossentropy', optimizer=optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw4QPoMNNEmn",
        "outputId": "95e85dcb-2e80-43a8-db2b-17fa7c4d647d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_7 (Dense)             (None, 256)               25856     \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 512)               131584    \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1025)              525825    \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 1025)              0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 1025)             4100      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 784)               804384    \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,494,821\n",
            "Trainable params: 1,491,235\n",
            "Non-trainable params: 3,586\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a GAN the Generator network takes noise z as an input to produce its images."
      ],
      "metadata": {
        "id": "79sKwgJJM9qE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = Input(shape=(100,))   #Our random input to the generator\n",
        "img = generator(z)"
      ],
      "metadata": {
        "id": "eoqRC_71NL3A"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, in below, it ensures that when we combine our networks we only train the Generator.\n",
        "While generator training we do not want discriminator weights to be adjusted. \n",
        "This Doesn't affect the above descriminator training. "
      ],
      "metadata": {
        "id": "PCipztfKN7Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator.trainable = False "
      ],
      "metadata": {
        "id": "d5SF1pFkP3EB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, it specifies that our Discriminator will take the images generated by our Generator and true dataset and set its output to a parameter called valid, which will indicate whether the input is real or not."
      ],
      "metadata": {
        "id": "NFKBAN0_QTYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid = discriminator(img)  #Validity check on the generated image"
      ],
      "metadata": {
        "id": "5P8dgjaLQXig"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we combined the models and also set our loss function and optimizer. \n",
        "Again, we are only training the generator here. \n",
        "The ultimate goal here is for the Generator to fool the Discriminator.  \n",
        "The combined model  (stacked generator and discriminator) takes:\n",
        "\n",
        "`noise as input >> generates images >> determines validity`"
      ],
      "metadata": {
        "id": "_JTB3M7Stamx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined = Model(z, valid)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
      ],
      "metadata": {
        "id": "mcBHFl6vQZnL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir images"
      ],
      "metadata": {
        "id": "yGZhElos4Qep"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(epochs=100, batch_size=32, save_interval=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npTKo4_atyym",
        "outputId": "a616da5b-8419-4a70-9ffc-9ae72217b94f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [D loss: 0.562613, acc.: 50.00%] [G loss: 0.523691]\n",
            "1 [D loss: 0.383469, acc.: 75.00%] [G loss: 0.552211]\n",
            "2 [D loss: 0.370355, acc.: 65.62%] [G loss: 0.584345]\n",
            "3 [D loss: 0.345811, acc.: 81.25%] [G loss: 0.726668]\n",
            "4 [D loss: 0.302275, acc.: 93.75%] [G loss: 0.877413]\n",
            "5 [D loss: 0.298396, acc.: 90.62%] [G loss: 1.047882]\n",
            "6 [D loss: 0.251725, acc.: 100.00%] [G loss: 1.372830]\n",
            "7 [D loss: 0.205758, acc.: 96.88%] [G loss: 1.498451]\n",
            "8 [D loss: 0.145196, acc.: 100.00%] [G loss: 1.730555]\n",
            "9 [D loss: 0.109174, acc.: 100.00%] [G loss: 1.810414]\n",
            "10 [D loss: 0.115538, acc.: 100.00%] [G loss: 2.023898]\n",
            "11 [D loss: 0.106259, acc.: 100.00%] [G loss: 2.088122]\n",
            "12 [D loss: 0.091836, acc.: 100.00%] [G loss: 2.288846]\n",
            "13 [D loss: 0.061167, acc.: 100.00%] [G loss: 2.499311]\n",
            "14 [D loss: 0.072475, acc.: 100.00%] [G loss: 2.527823]\n",
            "15 [D loss: 0.042798, acc.: 100.00%] [G loss: 2.706086]\n",
            "16 [D loss: 0.047104, acc.: 100.00%] [G loss: 2.687873]\n",
            "17 [D loss: 0.048316, acc.: 100.00%] [G loss: 2.835106]\n",
            "18 [D loss: 0.034857, acc.: 100.00%] [G loss: 2.787871]\n",
            "19 [D loss: 0.031990, acc.: 100.00%] [G loss: 2.975792]\n",
            "20 [D loss: 0.035091, acc.: 100.00%] [G loss: 2.977195]\n",
            "21 [D loss: 0.038606, acc.: 100.00%] [G loss: 3.028370]\n",
            "22 [D loss: 0.031522, acc.: 100.00%] [G loss: 3.114628]\n",
            "23 [D loss: 0.033209, acc.: 100.00%] [G loss: 3.227500]\n",
            "24 [D loss: 0.030814, acc.: 100.00%] [G loss: 3.280499]\n",
            "25 [D loss: 0.032394, acc.: 100.00%] [G loss: 3.319368]\n",
            "26 [D loss: 0.028152, acc.: 100.00%] [G loss: 3.351799]\n",
            "27 [D loss: 0.024172, acc.: 100.00%] [G loss: 3.311005]\n",
            "28 [D loss: 0.021524, acc.: 100.00%] [G loss: 3.432260]\n",
            "29 [D loss: 0.019056, acc.: 100.00%] [G loss: 3.580852]\n",
            "30 [D loss: 0.021486, acc.: 100.00%] [G loss: 3.579827]\n",
            "31 [D loss: 0.029821, acc.: 100.00%] [G loss: 3.607879]\n",
            "32 [D loss: 0.020702, acc.: 100.00%] [G loss: 3.516968]\n",
            "33 [D loss: 0.021222, acc.: 100.00%] [G loss: 3.715956]\n",
            "34 [D loss: 0.012999, acc.: 100.00%] [G loss: 3.703541]\n",
            "35 [D loss: 0.012391, acc.: 100.00%] [G loss: 3.684049]\n",
            "36 [D loss: 0.019769, acc.: 100.00%] [G loss: 3.807904]\n",
            "37 [D loss: 0.016795, acc.: 100.00%] [G loss: 3.768576]\n",
            "38 [D loss: 0.019962, acc.: 100.00%] [G loss: 3.748501]\n",
            "39 [D loss: 0.019239, acc.: 100.00%] [G loss: 3.835219]\n",
            "40 [D loss: 0.012408, acc.: 100.00%] [G loss: 3.836442]\n",
            "41 [D loss: 0.011038, acc.: 100.00%] [G loss: 3.919393]\n",
            "42 [D loss: 0.013436, acc.: 100.00%] [G loss: 3.760763]\n",
            "43 [D loss: 0.019026, acc.: 100.00%] [G loss: 3.945880]\n",
            "44 [D loss: 0.011143, acc.: 100.00%] [G loss: 4.014274]\n",
            "45 [D loss: 0.015866, acc.: 100.00%] [G loss: 4.050292]\n",
            "46 [D loss: 0.017199, acc.: 100.00%] [G loss: 4.062347]\n",
            "47 [D loss: 0.014049, acc.: 100.00%] [G loss: 4.155720]\n",
            "48 [D loss: 0.011896, acc.: 100.00%] [G loss: 4.119676]\n",
            "49 [D loss: 0.013179, acc.: 100.00%] [G loss: 3.974746]\n",
            "50 [D loss: 0.008654, acc.: 100.00%] [G loss: 4.054349]\n",
            "51 [D loss: 0.014154, acc.: 100.00%] [G loss: 4.151805]\n",
            "52 [D loss: 0.014711, acc.: 100.00%] [G loss: 4.272578]\n",
            "53 [D loss: 0.009518, acc.: 100.00%] [G loss: 4.149790]\n",
            "54 [D loss: 0.007584, acc.: 100.00%] [G loss: 4.131435]\n",
            "55 [D loss: 0.008817, acc.: 100.00%] [G loss: 4.069495]\n",
            "56 [D loss: 0.017426, acc.: 100.00%] [G loss: 4.214207]\n",
            "57 [D loss: 0.011018, acc.: 100.00%] [G loss: 4.108624]\n",
            "58 [D loss: 0.018917, acc.: 100.00%] [G loss: 4.104624]\n",
            "59 [D loss: 0.010089, acc.: 100.00%] [G loss: 4.214947]\n",
            "60 [D loss: 0.015685, acc.: 100.00%] [G loss: 4.207933]\n",
            "61 [D loss: 0.019368, acc.: 100.00%] [G loss: 4.444936]\n",
            "62 [D loss: 0.015142, acc.: 100.00%] [G loss: 4.556768]\n",
            "63 [D loss: 0.010619, acc.: 100.00%] [G loss: 4.566808]\n",
            "64 [D loss: 0.009764, acc.: 100.00%] [G loss: 4.477441]\n",
            "65 [D loss: 0.006732, acc.: 100.00%] [G loss: 4.542995]\n",
            "66 [D loss: 0.010500, acc.: 100.00%] [G loss: 4.502314]\n",
            "67 [D loss: 0.009925, acc.: 100.00%] [G loss: 4.545537]\n",
            "68 [D loss: 0.010362, acc.: 100.00%] [G loss: 4.465955]\n",
            "69 [D loss: 0.016148, acc.: 100.00%] [G loss: 4.629718]\n",
            "70 [D loss: 0.010177, acc.: 100.00%] [G loss: 4.732328]\n",
            "71 [D loss: 0.007144, acc.: 100.00%] [G loss: 4.556761]\n",
            "72 [D loss: 0.007726, acc.: 100.00%] [G loss: 4.556353]\n",
            "73 [D loss: 0.007340, acc.: 100.00%] [G loss: 4.593941]\n",
            "74 [D loss: 0.016670, acc.: 100.00%] [G loss: 4.462197]\n",
            "75 [D loss: 0.020874, acc.: 100.00%] [G loss: 4.573601]\n",
            "76 [D loss: 0.005206, acc.: 100.00%] [G loss: 4.687890]\n",
            "77 [D loss: 0.006074, acc.: 100.00%] [G loss: 4.792085]\n",
            "78 [D loss: 0.005121, acc.: 100.00%] [G loss: 4.459487]\n",
            "79 [D loss: 0.011736, acc.: 100.00%] [G loss: 4.528278]\n",
            "80 [D loss: 0.008016, acc.: 100.00%] [G loss: 4.579068]\n",
            "81 [D loss: 0.010146, acc.: 100.00%] [G loss: 4.599427]\n",
            "82 [D loss: 0.006668, acc.: 100.00%] [G loss: 4.418807]\n",
            "83 [D loss: 0.006792, acc.: 100.00%] [G loss: 4.412866]\n",
            "84 [D loss: 0.024011, acc.: 100.00%] [G loss: 4.531687]\n",
            "85 [D loss: 0.007736, acc.: 100.00%] [G loss: 4.735283]\n",
            "86 [D loss: 0.016594, acc.: 100.00%] [G loss: 4.718281]\n",
            "87 [D loss: 0.012794, acc.: 100.00%] [G loss: 4.679626]\n",
            "88 [D loss: 0.012097, acc.: 100.00%] [G loss: 4.774010]\n",
            "89 [D loss: 0.006394, acc.: 100.00%] [G loss: 4.675934]\n",
            "90 [D loss: 0.007741, acc.: 100.00%] [G loss: 4.735822]\n",
            "91 [D loss: 0.008037, acc.: 100.00%] [G loss: 4.600827]\n",
            "92 [D loss: 0.007346, acc.: 100.00%] [G loss: 4.837965]\n",
            "93 [D loss: 0.009150, acc.: 100.00%] [G loss: 4.625679]\n",
            "94 [D loss: 0.010313, acc.: 100.00%] [G loss: 4.662352]\n",
            "95 [D loss: 0.008521, acc.: 100.00%] [G loss: 4.865099]\n",
            "96 [D loss: 0.006227, acc.: 100.00%] [G loss: 4.961599]\n",
            "97 [D loss: 0.008175, acc.: 100.00%] [G loss: 4.910067]\n",
            "98 [D loss: 0.013931, acc.: 100.00%] [G loss: 4.952271]\n",
            "99 [D loss: 0.010984, acc.: 100.00%] [G loss: 4.753182]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save model for future use to generate fake images."
      ],
      "metadata": {
        "id": "l1-dZhf7t5Iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator.save('generator_model.h5')"
      ],
      "metadata": {
        "id": "moBdSy0huDYm"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}